<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 4 Procesos estacionarios univariados | Notas de Clase: Series de Tiempo</title>
  <meta name="description" content="Trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 4 Procesos estacionarios univariados | Notas de Clase: Series de Tiempo" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 4 Procesos estacionarios univariados | Notas de Clase: Series de Tiempo" />
  
  <meta name="twitter:description" content="Trabajo siempre en proceso de mejora, para cualquier comentario o aclaración, contactar al correo benjov@ciencias.unam.mx o omarxalpha@gmail.com" />
  

<meta name="author" content="Benjamín Oliva &amp; Omar Alfaro-Rivera" />


<meta name="date" content="2021-08-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modelos-de-series-de-tiempo-estacionarias.html"/>
<link rel="next" href="desestacionalización-y-filtrado-de-series.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Series de Tiempo</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#a-la-naturaleza-de-los-datos-de-series-de-tiempo"><i class="fa fa-check"></i><b>1.1</b> a) La naturaleza de los datos de Series de Tiempo</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#b-ejemplos-y-aplicaciones-de-las-series-de-tiempo"><i class="fa fa-check"></i><b>1.2</b> b) Ejemplos y aplicaciones de las Series de Tiempo</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html"><i class="fa fa-check"></i><b>2</b> Elementos de Ecuaciones en Diferencia</a><ul>
<li class="chapter" data-level="2.1" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html#a-ecuaciones-en-diferencia-para-procesos-deterministas"><i class="fa fa-check"></i><b>2.1</b> a) Ecuaciones en Diferencia para procesos deterministas</a><ul>
<li class="chapter" data-level="2.1.1" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html#ecuaciones-en-diferencia-lineales-de-primer-orden"><i class="fa fa-check"></i><b>2.1.1</b> Ecuaciones en Diferencia Lineales de Primer Orden</a></li>
<li class="chapter" data-level="2.1.2" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html#ecuaciones-en-diferencia-lineales-de-segundo-orden-y-de-orden-superior"><i class="fa fa-check"></i><b>2.1.2</b> Ecuaciones en Diferencia Lineales de Segundo Orden y de orden superior</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="elementos-de-ecuaciones-en-diferencia.html"><a href="elementos-de-ecuaciones-en-diferencia.html#operador-de-rezago-l"><i class="fa fa-check"></i><b>2.2</b> Operador de rezago L</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modelos-de-series-de-tiempo-estacionarias.html"><a href="modelos-de-series-de-tiempo-estacionarias.html"><i class="fa fa-check"></i><b>3</b> Modelos de Series de Tiempo Estacionarias</a><ul>
<li class="chapter" data-level="3.1" data-path="modelos-de-series-de-tiempo-estacionarias.html"><a href="modelos-de-series-de-tiempo-estacionarias.html#definición-de-ergodicidad-y-estacionariedad"><i class="fa fa-check"></i><b>3.1</b> Definición de ergodicidad y estacionariedad</a></li>
<li class="chapter" data-level="3.2" data-path="modelos-de-series-de-tiempo-estacionarias.html"><a href="modelos-de-series-de-tiempo-estacionarias.html#función-de-autocorrelación"><i class="fa fa-check"></i><b>3.2</b> Función de autocorrelación</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html"><i class="fa fa-check"></i><b>4</b> Procesos estacionarios univariados</a><ul>
<li class="chapter" data-level="4.1" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#procesos-autoregresivos-ar"><i class="fa fa-check"></i><b>4.1</b> Procesos Autoregresivos (AR)</a><ul>
<li class="chapter" data-level="4.1.1" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#ar1"><i class="fa fa-check"></i><b>4.1.1</b> AR(1)</a></li>
<li class="chapter" data-level="4.1.2" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#ar2"><i class="fa fa-check"></i><b>4.1.2</b> AR(2)</a></li>
<li class="chapter" data-level="4.1.3" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#arp"><i class="fa fa-check"></i><b>4.1.3</b> AR(p)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#procesos-de-medias-móviles-ma"><i class="fa fa-check"></i><b>4.2</b> Procesos de Medias Móviles (MA)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#ma1"><i class="fa fa-check"></i><b>4.2.1</b> MA(1)</a></li>
<li class="chapter" data-level="4.2.2" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#maq"><i class="fa fa-check"></i><b>4.2.2</b> MA(q)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#procesos-armap-q-y-arimap-d-q"><i class="fa fa-check"></i><b>4.3</b> Procesos ARMA(p, q) y ARIMA(p, d, q)</a><ul>
<li class="chapter" data-level="4.3.1" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#arma1-1"><i class="fa fa-check"></i><b>4.3.1</b> ARMA(1, 1)</a></li>
<li class="chapter" data-level="4.3.2" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#armap-q"><i class="fa fa-check"></i><b>4.3.2</b> ARMA(p, q)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#función-de-autocorrelación-parcial"><i class="fa fa-check"></i><b>4.4</b> Función de Autocorrelación Parcial</a></li>
<li class="chapter" data-level="4.5" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#selección-de-las-constantes-p-q-d-en-un-arp-un-maq-un-armap-q-o-un-arimap-d-q"><i class="fa fa-check"></i><b>4.5</b> Selección de las constantes p, q, d en un AR(p), un MA(q), un ARMA(p, q) o un ARIMA(p, d, q)</a></li>
<li class="chapter" data-level="4.6" data-path="procesos-estacionarios-univariados.html"><a href="procesos-estacionarios-univariados.html#pronósticos"><i class="fa fa-check"></i><b>4.6</b> Pronósticos</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="desestacionalización-y-filtrado-de-series.html"><a href="desestacionalización-y-filtrado-de-series.html"><i class="fa fa-check"></i><b>5</b> Desestacionalización y filtrado de Series</a></li>
<li class="chapter" data-level="6" data-path="procesos-basados-en-vectores-autoregresivos.html"><a href="procesos-basados-en-vectores-autoregresivos.html"><i class="fa fa-check"></i><b>6</b> Procesos Basados en Vectores Autoregresivos</a></li>
<li class="chapter" data-level="7" data-path="cointegración.html"><a href="cointegración.html"><i class="fa fa-check"></i><b>7</b> Cointegración</a></li>
<li class="chapter" data-level="8" data-path="modelos-multivariados-de-volatilidad-m-arch-y-m-garch.html"><a href="modelos-multivariados-de-volatilidad-m-arch-y-m-garch.html"><i class="fa fa-check"></i><b>8</b> Modelos multivariados de volatilidad: <span class="math inline">\(M - ARCH\)</span> y <span class="math inline">\(M - GARCH\)</span></a></li>
<li class="chapter" data-level="9" data-path="modelos-univariados-y-multivariados-de-volatilidad.html"><a href="modelos-univariados-y-multivariados-de-volatilidad.html"><i class="fa fa-check"></i><b>9</b> Modelos Univariados y Multivariados de Volatilidad</a></li>
<li class="chapter" data-level="10" data-path="modelos-adrl.html"><a href="modelos-adrl.html"><i class="fa fa-check"></i><b>10</b> Modelos ADRL</a></li>
<li class="chapter" data-level="11" data-path="modelos-de-datos-panel.html"><a href="modelos-de-datos-panel.html"><i class="fa fa-check"></i><b>11</b> Modelos de Datos Panel</a></li>
<li class="chapter" data-level="12" data-path="otros-modelos-de-series-de-tiempo-no-lineales.html"><a href="otros-modelos-de-series-de-tiempo-no-lineales.html"><i class="fa fa-check"></i><b>12</b> Otros Modelos de Series de Tiempo No lineales</a></li>
<li class="chapter" data-level="13" data-path="apendice-i.html"><a href="apendice-i.html"><i class="fa fa-check"></i><b>13</b> Apendice I</a><ul>
<li class="chapter" data-level="13.1" data-path="apendice-i.html"><a href="apendice-i.html#estimador-de-mínimos-cuadrados-ordinarios-y-el-análisis-clásico-de-regresión"><i class="fa fa-check"></i><b>13.1</b> Estimador de Mínimos Cuadrados Ordinarios y el análisis clásico de regresión</a></li>
<li class="chapter" data-level="13.2" data-path="apendice-i.html"><a href="apendice-i.html#estimación-por-el-método-de-máxima-verosimilitud-mv"><i class="fa fa-check"></i><b>13.2</b> Estimación por el método de Máxima Verosimilitud (MV)</a></li>
<li class="chapter" data-level="13.3" data-path="apendice-i.html"><a href="apendice-i.html#métricas-de-bondad-de-ajuste"><i class="fa fa-check"></i><b>13.3</b> Métricas de bondad de ajuste</a></li>
<li class="chapter" data-level="13.4" data-path="apendice-i.html"><a href="apendice-i.html#pruebas-de-hipótesis"><i class="fa fa-check"></i><b>13.4</b> Pruebas de Hipótesis</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/benjov/Series-de-Tiempo-2021" target="blank">Repositorio del Curso</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notas de Clase: Series de Tiempo</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="procesos-estacionarios-univariados" class="section level1">
<h1><span class="header-section-number">Capítulo 4</span> Procesos estacionarios univariados</h1>
<p>En este capítulo analizaremos el método o metodología de análisis de series de tiempo propuesto por Box y Jenkins (1970). Los modelos propuestos dentro de está metodología o conjunto de métodos se han vuelto indispensables para efectos de realizar pronósticos de corto plazo.</p>
<p>En este sentido, se analizarán los métodos más importantes en series de tiempo: Autoregresivos (AR) y de Medias Móviles (MA). Asimismo, se realizará un análisis de los procesos que resultan de la combinación de ambos, conocida como ARMA, los cuales son más comúnmente usados para realizar pronósticos.</p>
<div id="procesos-autoregresivos-ar" class="section level2">
<h2><span class="header-section-number">4.1</span> Procesos Autoregresivos (AR)</h2>
<p>Los procesos autoregresivos tienen su origen en el trabajo de Cochrane y Orcutt de 1949, mediante el cual analizaron los residuales de una regresión clásica como un proceso autoregresivo. Puede consultarse el apéndice para la discusión del modelo de regresión clásica.</p>
<div id="ar1" class="section level3">
<h3><span class="header-section-number">4.1.1</span> AR(1)</h3>
<p>Como primer caso analizaremos al proceso autoregresivo de primer orden, <span class="math inline">\(AR(1)\)</span>, el cual podemos definir como una Ecuación Lineal en Diferencia Estocástica de Primer Orden. Diremos que una Ecuación Lineal en Diferencia de Primer Orden es estocástica si en su representación analítica considera un componente estocástico como en la ecuación () descrita a continuación:
<span class="math display">\[\begin{equation}
    X_t = a_0 + a_1 X_{t-1} + U_t
    \label{EDO_Est}
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(a_0\)</span> es un término constante, <span class="math inline">\(U_t\)</span> es un proceso estacionario, con media cero (0), una varianza finita y constante (<span class="math inline">\(\sigma^2\)</span>) y una covarianza que depende de la distancia entre <span class="math inline">\(t\)</span> y cualquier <span class="math inline">\(t-s\)</span> (<span class="math inline">\(\gamma_s\)</span>)–que no depende de los valores pasados o futuros de la variable–, <span class="math inline">\(X_0\)</span> es el valor inicial de <span class="math inline">\(X_t\)</span>. No obstante, en general vamos a asumir que la covarianza será cero (0), por lo que tendremos un proceso puramente aleatorio. Considerando la ecuación ( y un proceso de sustitución sucesivo podemos establecer lo siguiente, empezando con <span class="math inline">\(X_1\)</span>:
<span class="math display">\[\begin{eqnarray*}
    X_{1} &amp; = &amp; a_0 + a_1 X_{0} + U_{1}
\end{eqnarray*}\]</span></p>
<p>Para <span class="math inline">\(X_2\)</span>:
<span class="math display">\[\begin{eqnarray*}
X_{2} &amp; = &amp; a_0 + a_1 X_{1} + U_{2} \\
    &amp; = &amp; a_0 + a_1 (a_0 + a_1 X_{0} + U_{1}) + U_{2} \\
    &amp; = &amp; a_0 + a_1 a_0 + a_1^2 X_{0} + a_1 U_{1} + U_{2}
\end{eqnarray*}\]</span></p>
<p>Para <span class="math inline">\(X_3\)</span>:
<span class="math display">\[\begin{eqnarray*}
X_{3} &amp; = &amp; a_0 + \alpha X_{2} + U_{3} \\
    &amp; = &amp; a_0 + a_1 (a_0 + a_1 a_0 + a_1^2 X_{0} + a_1 U_{1} + U_{2}) + U_{3} \\
    &amp; = &amp; a_0 + a_1 a_0 + a_1^2 a_0 + a_1^3 X_{0} + a_1^2 U_{1} + a_1 U_{2} + U_{3}
\end{eqnarray*}\]</span></p>
<p>Así, para cualquier <span class="math inline">\(X_t\)</span>, <span class="math inline">\(t = 1, 2, 3, \ldots\)</span>, obtendríamos:
<span class="math display">\[\begin{eqnarray}
X_{t} &amp; = &amp; a_0 + a_1 X_{t - 1} + U_{t} \nonumber \\
    &amp; = &amp; a_0 + a_1 (a_0 + a_1 a_0 + a_1^2 a_0 + \ldots + a_1^{t-2} a_0 + a_1^{t-1} X_{0} \nonumber \\
    &amp;   &amp; + a_1^{t-2} U_{1} + \ldots + a_1 U_{t - 2} + U_{t - 1}) + U_{t} \nonumber \\
    &amp; = &amp; a_0 + a_1 a_0 + a_1^2 a_0 + a_1^3 a_0 + \ldots + a_1^{t-1} a_0 + a_1^{t} X_{0} \nonumber \\
    &amp;   &amp; + a_1^{t-1} U_{1} + \ldots a_1^2 U_{t - 2} + a_1 U_{t - 1} + U_{t} \nonumber \\
    &amp; = &amp; (1 + a_1 + a_1^2 + a_1^3 + \ldots + a_1^{t-1}) a_0 + a_1^{t} X_{0} \nonumber \\
    &amp;   &amp; + a_1^{t-1} U_{1} + \ldots + a_1^2 U_{t - 2} + a_1 U_{t - 1} + U_{t}  \nonumber\\
    &amp; = &amp; \frac{1 - a_1^t}{1 - a_1} a_0 + a_1^{t} X_{0} + \sum^{t-1}_{j = 0} a_1^{j} U_{t - j} 
    \label{EDO_S_Sol}
\end{eqnarray}\]</span></p>
<p>De esta forma en la ecuación () observamos un proceso que es explicado por dos partes: una que depende del tiempo y otra que depende de un proceso estocástico. Asimismo, debe notarse que la condición de convergencia es idéntica que en el caso de ecuaciones en diferencia estudiadas al inicio del curso: <span class="math inline">\(\abs{a_1} &lt; 1\)</span>, por lo que cuando <span class="math inline">\(t \to \infty\)</span>, la expresión () será la siguiente:
<span class="math display">\[\begin{equation}
    X_t = \frac{1}{1 - a_1} a_0 + \sum^{\infty}_{j = 0} a_1^{j} U_{t - j}
    \label{EDO_S_LP}
\end{equation}\]</span></p>
<p>Así, desaparece la parte dependiente del tiempo y únicamente prevalece la parte que es dependiente del proceso estocástico. Esta es la solución de largo plazo del proceso <span class="math inline">\(AR(1)\)</span>, la cual depende del proceso estocástico. Notemos, además, que esta solución implica que la variable o la serie de tiempo <span class="math inline">\(X_t\)</span> es tambien un proceso estocástico que hereda las propiedades de <span class="math inline">\(U_t\)</span>. Así, <span class="math inline">\(X_t\)</span> es también un proceso estocástico estacionario, como demostraremos más adelante.</p>
<p>Observemos que la ecuación () se puede reescribir si consideramos la formulación que en la literatura se denomina como la descomposición de Wold, en la cual se define que es posible asumir que <span class="math inline">\(\psi_j = a_1^j\)</span> y se considera el caso en el cual <span class="math inline">\(\abs{a_1} &lt; 1\)</span>, de esta forma tendremos que por ejemplo cuando:
<span class="math display">\[\begin{equation*}
    \sum^{\infty}_{j = 0} \psi^2_j = \sum^{\infty}_{j = 0} a_1^{2j} = \frac{1}{1 - a_1^2} 
\end{equation*}\]</span></p>
<p>Alternativamente y de forma similar a las ecuaciones en diferencia estudiadas previamente podemos escribir el proceso <span class="math inline">\(AR(1)\)</span> mediante el uso del operador rezago como:
<span class="math display">\[\begin{eqnarray}
    X_t &amp; = &amp; a_0 + a_1 L X_t + U_t \nonumber \\
    X_t - a_1 L X_t &amp; = &amp; a_0 + U_t \nonumber \\
    (1 - a_1 L) X_t &amp; = &amp; a_0 + U_t \nonumber \\
    X_t &amp; = &amp; \frac{a_0}{1 - a_1 L} + \frac{1}{1 - a_1 L} U_t
    \label{AR_1}
\end{eqnarray}\]</span></p>
<p>En esta última ecuación retomamos el siguiente término para reescribirlo como:
<span class="math display">\[\begin{equation}
    \frac{1}{1 - a_1 L} = 1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \ldots 
\end{equation}\]</span></p>
<p>Tomando este resultado para sustituirlo en ecuación (), obtenemos la siguiente expresión:
<span class="math display">\[\begin{eqnarray}
X_t &amp; = &amp; (1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \ldots) a_0 + (1 + a_1 L + a_1^2 L^2 + a_1^3 L^3 + \ldots) U_t \nonumber \\
    &amp; = &amp; (1 + a_1 + a_1^2 + a_1^3 + \ldots) a_0 + U_t + a_1 U_{t-1} + a_1^2 U_{t-2} + a_1^3 U_{t-3} + \ldots \nonumber \\
    &amp; = &amp; \frac{a_0}{1 - a_1} + \sum^{\infty}_{j = 0} a_1^j U_{t-j}
    \label{AR1_Sol}
\end{eqnarray}\]</span></p>
<p>Donde la condición de convergencia y estabilidad del proceso descrito en esta ecuación es que <span class="math inline">\(\abs{a_1} &lt; 1\)</span>. Por lo que hemos demostrado que mediante el uso del operador de rezago es posible llegar al mismo resultado que obtuvimos mediante el procedimiento de sustituciones iterativas.</p>
<p>La ecuación () se puede interpretar como sigue. La solución o trayectoria de equilibrio de un AR(1) se divide en dos partes. La primera es una constante que depende de los valores de <span class="math inline">\(a_0\)</span> y <span class="math inline">\(a_1\)</span>. La segunda parte es la suma ponderada de las desviaciones o errores observados y acumulados en el tiempo hasta el momento <span class="math inline">\(t\)</span>.</p>
Ahora obtendremos los momentos que describen a la serie de tiempo cuando se trata de un porceso <span class="math inline">\(AR(1)\)</span>. Para ello debemos obtener la media, la varianza y las covarianzas de <span class="math inline">\(X_t\)</span>. Para los siguientes resultados debemos recordar y tener en mente que si <span class="math inline">\(U_t\)</span> es un proceso puramente aleatorio, entonces:

<p>Dicho lo anterior y partiendo de la ecuación (), el primer momento o valor esperado de la serie de tiempo será el siguiente:
<span class="math display">\[\begin{eqnarray}
\mathbb{E}[X_t] &amp; = &amp; \mathbb{E} \left[ \frac{a_0}{1 - a_1} + \sum^{\infty}_{j = 0} a_1^j U_{t-j} \right] \nonumber \\
    &amp; = &amp; \frac{a_0}{1 - a_1} + \sum^{\infty}_{j = 0} a_1^j \mathbb{E}[U_{t-j}] \nonumber \\
    &amp; = &amp; \frac{a_0}{1 - a_1} = \mu
    \label{AR1_m1}
\end{eqnarray}\]</span></p>
<p>Respecto de la varianza podemos escribir la siguiente expresión a partir de la ecuación ():
<span class="math display">\[\begin{eqnarray}
Var[X_t] &amp; = &amp; \mathbb{E}[(X_t - \mu)^2] \nonumber \\
    &amp; = &amp; \mathbb{E} \left[ \left( \frac{a_0}{1 - a_1} + \sum^{\infty}_{j = 0} a_1^j U_{t-j} - \frac{a_0}{1 - a_1} \right)^2 \right] \nonumber \\
    &amp; = &amp; \mathbb{E}[(U_{t} + a_1 U_{t-1} + a_1^2 U_{t-2} + a_1^3 U_{t-3} + \ldots)^2] \nonumber \\
    &amp; = &amp; \mathbb{E}[U^2_{t} + a_1^2 U^2_{t-1} + a_1^4 U^2_{t-2} + a_1^6 U^2_{t-3} + \ldots \nonumber \\
    &amp;   &amp; + 2 a_1 U_t U_{t-1} + 2 a_1^2 U_t U_{t-2} + \ldots] \nonumber \\
    &amp; = &amp; \mathbb{E}[U^2_{t}] + a_1^2 \mathbb{E}[U^2_{t-1}] + a_1^4 \mathbb{E}[U^2_{t-2}] + a_1^6 \mathbb{E}[U^2_{t-3}] + \ldots \nonumber \\
    &amp; = &amp; \sigma^2 + a_1^2 \sigma^2 + a_1^4 \sigma^2 + a_1^6 \sigma^2 + \ldots \nonumber \\
    &amp; = &amp; \sigma^2 (1 + a_1^2 + a_1^4 + a_1^6 + \ldots) \nonumber \\
    &amp; = &amp; \sigma^2 \frac{1}{1 - a_1^2} = \gamma(0)
    \label{AR1_Var}
\end{eqnarray}\]</span></p>
<p>Previo a analizar la covarianza de la serie recordemos que para el proceso puramente aleatorio <span class="math inline">\(U_t\)</span> su varianza y covarianza puede verse como <span class="math inline">\(\mathbb{E}[U_t, U_s] = \sigma^2\)</span>, para <span class="math inline">\(t = s\)</span>, y <span class="math inline">\(\mathbb{E}[U_t, U_s] = 0\)</span>, para cualquier otro caso, respectivamente.</p>
<p>Dicho lo anterior, partiendo de la ecuación () la covarianza de la serie estará dada por:
<span class="math display">\[\begin{eqnarray}
Cov(X_t, X_{t-\tau}) &amp; = &amp; \mathbb{E}[(X_t - \mu)(X_{t-\tau} - \mu)] \nonumber \\
    &amp; = &amp; \mathbb{E} \left[ \left( \frac{a_0}{1 - a_1} + \sum^{\infty}_{j = 0} a_1^j U_{t-j} - \frac{a_0}{1 - a_1} \right) \right. \nonumber \\
    &amp;   &amp; \left. \times \left( \frac{a_0}{1 - a_1} + \sum^{\infty}_{j = 0} a_1^j U_{t-\tau-j} - \frac{a_0}{1 - a_1} \right) \right] \nonumber \\
    &amp; = &amp; a_1^{\tau} \mathbb{E}[U^2_{t-\tau} + a_1 U^2_{t-\tau-1} + a_1^2 U^2_{t-\tau-2} + a_1^3 U^2_{t-\tau-3} + \ldots] \nonumber \\
    &amp; = &amp; a_1^{\tau} \sigma^2 \frac{1}{1 - a_1^2} = \gamma(\tau)
    \label{AR1_Cov}
\end{eqnarray}\]</span></p>
<p>Notése que con estos resultados en las ecuaciones () y () podemos construir la función de autocorrelación teórica como sigue:
<span class="math display">\[\begin{eqnarray}
\rho(\tau) &amp; = &amp; \frac{\gamma(\tau)}{\gamma(0)} \nonumber \\
    &amp; = &amp; a_1^\tau
\end{eqnarray}\]</span></p>
<p>Donde <span class="math inline">\(\tau = 1, 2, 3, \ldots\)</span> y <span class="math inline">\(\abs{a_1} &lt; 1\)</span>. Este último resultado significa que cuando el proceso autoregresivo es de orden 1 (es decir, AR(1)) la función de autocorrelación teóricamente es igual al parámetro <span class="math inline">\(a_1\)</span> elevado al número de rezagos considerados. No obstante, note que esto no significa que la autocorrelación observada sea como lo expresa en planteamiento anterior. Por el contrario, una observación sencilla mostraría que la autocorrelación observada sería ligeramente distinta a la autocorrelación teórica.</p>
<p>Ahora veámos algunos ejemplos. En el primer ejemplo simularemos una serie y mostraremos el analísis de un proceso construído considerando un proceso puramente aleatorio como componente <span class="math inline">\(U_t\)</span>. Por su parte, en un segundo ejemplo aplicaremos el análisis a una serie de tiempo de una variable económica observada.</p>
<p>Para el primer ejemplo consideremos un proceso dado por la forma de un <span class="math inline">\(AR(1)\)</span> como en la ecuación () cuya solución esta dada por la ecuación (). En especifico, supongamos que el término o componente estocástico <span class="math inline">\(U_t\)</span> es una serie generada a partir de numeros aleatorios de una función normal con media <span class="math inline">\(0\)</span> y desviación estándar <span class="math inline">\(4\)</span>. Los detalles del proceso simulado se muestra en las siguientes gráficas.</p>
La Figura  ilustra el comportamiento que se debería observar en una serie considerando el procedimiento iterativo de construcción. Por su parte, la Figura  ilustra el proceso o trayectoria de la solución de la serie de tiempo. Finalmente, las Figuras  y  muestran el correlograma calculado considerando una función de autocorrelación aplicada al porceso real y una función de autocorrelación aplicada al proceso teórico, respectivamente.




Recordemos que una trayectoria de equilibrio o solución de un <span class="math inline">\(AR(1)\)</span> es como se muestra en la ecuación (). Así, nuestra serie simulada cumple con la característica de que los errores son más relevantes cuando la serie es corta. Por el contrario, los errores son menos relevantes, cuando la serie es muy larga. La Figura  ilustra esta observación de la trayectoria de equilibrio.

<p>Para el segundo ejemplo consideremos una aplicación a una serie de tiempo en especifico: Pasajeros transportados mensualmente en el Sistema de Transporte Colectivo Metro (pasajeros medidos en millones).</p>
<p>A la serie se le aplicará una metodología de estimación dada por el método de Máxima Verosimilitud (ML, por sus siglás en inglés). Antes de realizar el proceso de estimación consideremos una transformación de diferencias logaritmicas, con el objeto de obtener una serie de tiempo expresada en tasas de crecimiento y con un comportamiento parecido a un proceso estacionario.</p>
<p>Así, para cada una de las series que analicemos en diferencias logaritmicas las expresaremos bajo la siguiente transformación: <span class="math display">\[\begin{equation*}
    DLX_t = log(X_t) - log(X_{t-k})
\end{equation*}\]</span></p>
<p>Donde <span class="math inline">\(k = 1, 2, 3, \ldots\)</span> y <span class="math inline">\(log(.)\)</span> es la función logaritmo natural. Esta expresión se pude interpretar como una tasa de crecimiento puesto que asumimos variaciones pequeñas para las cuales se cumple que: <span class="math inline">\(log(X_t) - log(X_{t-k}) \approx \frac{X_t - X_{t-k}}{X_t}\)</span>.</p>
<p>Primero, al realizar el análisis de una serie de tiempo deberemos decidir si éste se realizará para la serie en niveles o en diferencias. Por convención, decimos que la series esta en niveles si ésta se analiza sin heacerle ninguna transformación o si se analiza aplicando logarimos. Cuando la serie se analiza en diferencias significa que la diferencia se hace sin aplicar logaritmos o aplicando logaritmos. Sin embargo, lo común es hacer un análisis en logaritmos.</p>
Para decidir cómo analizar la serie de pasajeros en el metro de la CDMX en la Figura  se muestra la gráfica de la serie en niveles (sin transformación logaritmica y con transformación logarítmica) y en diferencias logarítmicas mensuales (es decir, con <span class="math inline">\(k = 1\)</span>).

A continuación, estimaremos una <span class="math inline">\(AR(1)\)</span> para la serie en niveles bajo la transformación logaritmica (<span class="math inline">\(PaxLMetro_t\)</span>) y en diferencias logarítmitcas (<span class="math inline">\(PaxDLMetro_t\)</span>). Para el primer caso obtenemos el siguiente resultado:


Para el segundo caso obtenemos el siguiente resultado:


<p>En ambos casos observamos que el parámetro asociado al componente AR es significativo y cumple con la restricción de ser en valor absoluto menor a 1, por lo que la solución asociada al procesp será convergente. También en ambos casos se reporta la estadística o Criterio de Información de Akaike (AIC, por sus siglas en inglés), misma que más adelante discutiremos su importancia y aplicación.</p>
</div>
<div id="ar2" class="section level3">
<h3><span class="header-section-number">4.1.2</span> AR(2)</h3>
<p>Una vez analizado el caso de <span class="math inline">\(AR(1)\)</span> analizaremos el caso del <span class="math inline">\(AR(2)\)</span>. La ecuación generalizada del proceso autoregresivo de orden 2 (denotado como <span class="math inline">\(AR(2)\)</span>) puede ser escrito como:
<span class="math display">\[\begin{equation}
    X_t = a_0 + a_1 X_{t-1} + a_2 X_{t-2} + U_t
    \label{AR2_Eq}
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(U_t\)</span> denota un proceso puramente aleatorio con media cero (<span class="math inline">\(0\)</span>), varianza constante (<span class="math inline">\(\sigma^2\)</span>) y autocovarianza cero (<span class="math inline">\(Cov(U_t, U_s) = 0\)</span>, con <span class="math inline">\(t \neq s\)</span>), y un parametro <span class="math inline">\(a_2 \neq 0\)</span>. Así, utilizando el operador rezago podemos reescribir la ecuación () como:
<span class="math display">\[\begin{eqnarray*}
    X_t - a_1 X_{t-1} - a_2 X_{t-2} &amp; = &amp; a_0 + U_t \\
    (1 - a_1 L^1 - a_2 L^2) X_t &amp; = &amp; a_0 + U_t
\end{eqnarray*}\]</span></p>
<p>Donde, vamos a denotar a <span class="math inline">\(\alpha (L) = (1 - a_1 L^1 - a_2 L^2)\)</span>, y lo llamaremos como un polinomio que depende del operador rezago y que es distinto de cero. De esta forma podemos reescribir a la ecuación () como:
<span class="math display">\[\begin{equation}
    \alpha(L) X_t = a_0 + U_t
\end{equation}\]</span></p>
<p>Ahora, supongamos que existe el inverso multiplicativo del polinomio <span class="math inline">\(\alpha(L)\)</span>, el cual será denotado como: <span class="math inline">\(\alpha^{-1}(L)\)</span> y cumple con que:
<span class="math display">\[\begin{equation}
    \alpha^{-1}(L) \alpha(L) = 1    
\end{equation}\]</span></p>
<p>Así, podemos escribir la solución a la ecuación () como:
<span class="math display">\[\begin{equation*}
    X_t = \alpha^{-1}(L) \delta + \alpha^{-1}(L) U_t
\end{equation*}\]</span></p>
<p>Si utilizamos el hecho que <span class="math inline">\(\alpha^{-1}(L)\)</span> se puede descomponer a través del procedimiento de Wold en un polinomio de forma similar el caso de <span class="math inline">\(AR(1)\)</span>, tenemos que:
<span class="math display">\[\begin{equation}
    \alpha^{-1}(L) = \psi_0 + \psi_1 L + \psi_2 L^2 + \ldots
\end{equation}\]</span></p>
<p>Por lo tanto, el inverso multiplicativo <span class="math inline">\(\alpha^{-1}(L)\)</span> se puede ver como:
<span class="math display">\[\begin{equation}
    1 = (1 - a_1 L^1 - a_2 L^2) (\psi_0 + \psi_1 L + \psi_2 L^2 + \ldots)
    \label{InvAlpha}
\end{equation}\]</span></p>
Desarrollando la ecuación () tenemos la sigueinte expresión:

Ahora, podemos agrupar todos los términos en función del exponente asociado al operador rezago <span class="math inline">\(L\)</span>. La siguiente es una solución partícular y es una de las múltiples que podrían existir que cumpla con la ecuación (). Sin embargo, para efectos del análisis sólo necesitamos una de esas soluciones. Utilizaremos las siguientes condiciones que deben cumplirse en una de las posibles soluciones:

<p>De esta forma podemos observar que en el límite siempre obtendremos una ecuación del tipo <span class="math inline">\(\psi_j - a_1 \psi_{j-1} - a_2 \psi_{j-2} = 0\)</span> asociada a cada uno de los casos en que exista un <span class="math inline">\(L^j\)</span>, donde <span class="math inline">\(j \neq 0, 1\)</span>, y la cual siempre podremos resolver conociendo que las condiciones iniciales son: <span class="math inline">\(\psi_0 = 1\)</span> y <span class="math inline">\(\psi_1 = a_1\)</span>.</p>
<p>Así, de las relaciones antes mencionadas y considerando que <span class="math inline">\(\alpha^{-1} (L)\)</span> aplicada a una constante como <span class="math inline">\(a_0\)</span>, tendrá como resultado otra constante. De esta forma podemos escribir que la solución del proceso AR(2) en la ecuación () será dada por una expresión como sigue:
<span class="math display">\[\begin{equation}
    X_t = \frac{\delta}{1 - a_1 - a_2} + \sum^{\infty}_{j = 0} \psi_{t - j} U_{t - j}
    \label{AR2_Eq_Sol}
\end{equation}\]</span></p>
<p>Donde todos los parametros <span class="math inline">\(\psi_i\)</span> está determinado por los parámtros <span class="math inline">\(a_0\)</span>, <span class="math inline">\(a_1\)</span> y <span class="math inline">\(a_2\)</span>. En particular, <span class="math inline">\(\psi_0 = 1\)</span> y <span class="math inline">\(\psi_1 = a_1\)</span> como describimos anteriormente. Al igual que en el caso del <span class="math inline">\(AR(1)\)</span>, en la ecuación () las condiciones de estabilidad estarán dadas por las soluciones del siguiente polinomio característico:
<span class="math display">\[\begin{equation}
    \lambda^2 - \lambda a_1 - a_2 = 0
\end{equation}\]</span></p>
<p>Así, la condición de estabilidad de la trayectoria es que <span class="math inline">\(\abs{\lambda_i} &lt; 1\)</span>, para <span class="math inline">\(i = 1, 2\)</span>. Es decir, es necesario que cada una de las raíces sea, en valor absoluto, siempre menor que la unidad. Estas son las condiciones de estabilidad para el proceso <span class="math inline">\(AR(2)\)</span>.</p>
<p>Finalmente, al igual que en un <span class="math inline">\(AR(1)\)</span>, a continuación determinamos los momentos de una serie que sigue un proceso <span class="math inline">\(AR(2)\)</span>. Iniciamos con la determinación de la media de la serie:
<span class="math display">\[\begin{equation}
    \mathbb{E}[X_t] = \mu = \frac{a_0}{1 - a_1 - a_2}
\end{equation}\]</span></p>
<p>Lo anterior es cierto puesto que <span class="math inline">\(\mathbb{E}[U_{t - i}] = 0\)</span>, para todo <span class="math inline">\(i = 0, 1, 2, \ldots\)</span>. Para determinar la varianza utilizaremos las siguientes relaciones basadas en el uso del valor esperado, varianza y covarianza de la serie. Adicionalmente, para simplificar el trabajo asumamos que <span class="math inline">\(a_0 = 0\)</span>, lo cual implica que <span class="math inline">\(\mu = 0\)</span>. Dicho lo anterior, partamos de:
<span class="math display">\[\begin{eqnarray*}
    \mathbb{E}[X_t X_{t - \tau}] &amp; = &amp; \mathbb{E}[(a_1 X_{t-1} + a_2 X_{t-2} + U_t) X_{t - \tau}]\\
    &amp; = &amp; a_1 \mathbb{E}[X_{t - 1} X_{t - \tau}] + a_2 \mathbb{E}[X_{t - 2} X_{t - \tau}] + \mathbb{E}[U_{t} X_{t - \tau}]
\end{eqnarray*}\]</span></p>
Donde <span class="math inline">\(\tau = 0, 1, 2, 3, \ldots\)</span> y que <span class="math inline">\(\mathbb{E}[U_{t} X_{t - \tau}] = 0\)</span> para todo <span class="math inline">\(\tau \neq 0\)</span>. Dicho esto, podemos derivar el valor del valor esperado para diferentes valores de <span class="math inline">\(\tau\)</span>:

<p>Donde debe ser claro que <span class="math inline">\(\mathbb{E}[(X_{t} - \mu)(X_{t - \tau} - \mu)] = \mathbb{E}[X_{t} X_{t - \tau}] = \gamma(\tau)\)</span>. Así, en general cuando <span class="math inline">\(\tau \neq 0\)</span>:
<span class="math display">\[\begin{equation}
    \gamma(\tau) = a_1 \gamma(\tau - 1) + a_2 \gamma(\tau - 2)
\end{equation}\]</span></p>
<p>Realizando la sustitución recursiva y solucionando el sistema respectivo obtenemos que las varianza y covarianzas estaran determinadas por:
<span class="math display">\[\begin{equation}
    Var[X_t] = \gamma(0) = \frac{1 - a_2}{(1 + a_2)[(1 - a_2)^2 - a^2_1]} \sigma^2
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
    \gamma(1) = \frac{a_1}{(1 + a_2)[(1 - a_2)^2 - a^2_1]} \sigma^2
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
    \gamma(2) = \frac{a^2_1 + a_2 - a^2_2}{(1 + a_2)[(1 - a_2)^2 - a^2_1]} \sigma^2
\end{equation}\]</span></p>
<p>Recordemos que las funciones de autocorrelación se obtienen de la división de cada unas de las funciones de covarianza (<span class="math inline">\(\gamma(\tau)\)</span>) por la varianza (<span class="math inline">\(\gamma(0)\)</span>). Así, podemos construir la siguiente expresión:
<span class="math display">\[\begin{equation}
    \rho(\tau) - a_1 \rho(\tau - 1) - a_2 \rho(\tau - 2) = 0
\end{equation}\]</span></p>
<p>Ahora veámos un ejemplo. Utilizaremos la serie de Pasajeros en vuelos nacionales (en vuelos de salidas) para estimar un <span class="math inline">\(AR(2)\)</span> mediante el método de máxima verosimilitud (ML, por sus siglas en inglés). Antes de realizar el proceso de estimación consideremos una transformación de la serie en logaritmos y una más en diferencias logarítmicas; lo anterior con el objeto de obtener un conjunto de series de tiempo suavizada y expresada en tasas de crecimiento, con un comportamiento parecido a un proceso estacionario.</p>
<p>Así, para cada una de las series que analicemos en diferencias logarítmicas las expresaremos bajo la siguiente transformación:
<span class="math display">\[\begin{equation*}
    DLX_t = log(X_t) - log(X_{t-k})
\end{equation*}\]</span></p>
<p>Donde <span class="math inline">\(k = 1, 2, 3, \ldots\)</span> y <span class="math inline">\(log(.)\)</span> es la función logaritmo natural. Por convención, decimos que la serie está en niveles si ésta se analiza sin heacerle ninguna transformación o se analiza en logarimos. Cuando la serie se analiza en diferencias significa que la diferencia se hace sin aplicar logaritmos. Y cuando la serie analizada está en diferenncias logarítmicas también diremos que esta en diferencias. Sin embargo, lo común es hacer un análisis en logaritmos y en difereencias logarítmicas.</p>
Primero, para decidir si se realizará un AR(2) para la serie en niveles o en diferencias analizaremos su gráfica. La serie en niveles, en niveles bajo una transformación logarítmica y en diferencias logarítmicas mensuales de los pasajeros en vuelos nacionales se muestra en la Figura .

A continuación, estimaremos un <span class="math inline">\(AR(2)\)</span> para la serie en niveles bajo una transformación logarítmica (<span class="math inline">\(LPaxNal_t\)</span>) y en diferencias logarítmitcas (<span class="math inline">\(DLPax_Nal_t\)</span>). Para el primer caso obtenemos el siguiente resultado:


Para el segundo caso obtenemos el siguiente resultado:


Para ambos casos entre parentésis indicamos los errores estándar y reportamos el estadístico de Akaike, AIC. Finalmente, podemos determinar si las soluciones serán convergentes, para ello en la Figura  mostramos las raíces asociadas a cada uno de los polinomios. De la inspección visual podemos concluir que ambas propuesta de AR(2) representan una solución convergente y estable.

</div>
<div id="arp" class="section level3">
<h3><span class="header-section-number">4.1.3</span> AR(p)</h3>
<p>Veremos ahora una generalización de los procesos autoregresivos (AR). Esta generalización es conocida como un proceso <span class="math inline">\(AR(p)\)</span> y que puede ser descrito por la siguiente ecuación en diferencia estocástica:
<span class="math display">\[\begin{equation}
    X_t = a_0 + a_1 X_{t-1} + a_2 X_{t-2} + a_3 X_{t-3} + \ldots + a_p X_{t-p} + U_t
    \label{ARp_Eq}
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(a_p \neq 0\)</span>, y <span class="math inline">\(U_t\)</span> es un proceso puramente aleatorio con media cero (0), varianza constante (<span class="math inline">\(\sigma^2\)</span>) y covarianza cero (0). Usando el operador rezago, <span class="math inline">\(L^k\)</span>, para <span class="math inline">\(k = 0, 1, 2, \ldots, p\)</span>, obtenemos la siguiente expresión de la ecuación ():
<span class="math display">\[\begin{equation}
    (1 - a_1 L - a_2 L^2 - a_3 L^3 - \ldots - a_p L^p) X_t = a_0 + U_t
\end{equation}\]</span></p>
<p>Definamos el polinomio <span class="math inline">\(\alpha(L)\)</span> como:
<span class="math display">\[\begin{equation}
    \alpha(L) = 1 - a_1 L - a_2 L^2 - a_3 L^3 - \ldots - a_p L^p
    \label{Pol_A}
\end{equation}\]</span></p>
<p>De forma similar que en los procesos <span class="math inline">\(AR(1)\)</span> y <span class="math inline">\(AR(2)\)</span>, las condiciones de estabilidad del proceso <span class="math inline">\(AR(p)\)</span> estarán dadas por la solución de la ecuación característica:
<span class="math display">\[\begin{equation}
    \lambda^p - a_1 \lambda^{p-1} - a_2 \lambda^{p-2} - a_3 \lambda^{p-3} - \ldots - a_p = 0
\end{equation}\]</span></p>
<p>Así, solo si el polinomio anterior tiene raíces cuyo valor absoluto sea menor a uno (<span class="math inline">\(\abs{\lambda_i} &lt; 1\)</span>) y si <span class="math inline">\(1 - a_1 L - a_2 L^2 - a_3 L^3 - \ldots - a_p L^p &lt; 1\)</span> podremos decir que el proceso es convergente y estable. Lo anterior significa que la ecuación () puede expresarse en términos de la descomposición de Wold o como la suma infinita de términos como:
<span class="math display">\[\begin{equation}
    \frac{1}{1 - a_1 L  - a_2 L^2 - a_3 L^3  - \ldots - a_p L^p} = \psi_0 + \psi_1 L + \psi_2 L^2 + \psi_3 L^3 + \ldots
\end{equation}\]</span></p>
<p>Donde, por construcción de <span class="math inline">\(\alpha(L) \alpha^{-1}(L) = 1\)</span> implica que <span class="math inline">\(\psi_0 = 1\)</span>. De forma similar a los proceso AR(1) y AR(2), es posible determinar el valor de los coefieentes <span class="math inline">\(\psi_j\)</span> en términos de los coefientes <span class="math inline">\(a_i\)</span>. Así, la solución del proceso <span class="math inline">\(AR(p)\)</span> estará dada por:
<span class="math display">\[\begin{equation}
    X_t = \frac{a_0}{1 - a_1  - a_2 - a_3  - \ldots - a_p} + \sum^{\infty}_{j = 0} \psi_j U_{t-j}
    \label{ARp_Eq_Sol}
\end{equation}\]</span></p>
<p>Considerando la solución de la ecuación () expresada en la ecuación () podemos determinar los momentos del proceso y que estarán dados por una media como:
<span class="math display">\[\begin{equation}
    \mathbb{E}[X_t] = \mu = \frac{a_o}{1 - a_1  - a_2 - a_3  - \ldots - a_p}
\end{equation}\]</span></p>
<p>Lo anterior, considerado que <span class="math inline">\(\mathbb{E}[U_t] = 0\)</span>, para todo <span class="math inline">\(t\)</span>. Para determinar la varianza del proceso, sin pérdida de generalidad, podemos definir una ecuación: <span class="math inline">\(\gamma(\tau) = \mathbb{E}[X_{t - \tau} X_t]\)</span>, la cual (omitiendo la constante, ya que la correlación de una constante con cuaquier variable aleatoria que depende del tiempo es cero (0)) puede ser escrita como:
<span class="math display">\[\begin{equation}
    \gamma(\tau) = \mathbb{E}[(X_{t - \tau}) \cdot (a_1 X_{t-1} + a_2 X_{t-2} + a_3 X_{t-3} + \ldots + + a_p X_{t-p} + U_t)]
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(\tau = 0, 1, 2, \ldots, p\)</span> y <span class="math inline">\(a_0 = 0\)</span>, lo que implica que <span class="math inline">\(\mu = 0\)</span>. De lo anterior obtenemos el siguiente conjunto de ecuaciones mediante sustituciones de los valores de <span class="math inline">\(\tau\)</span>:
<span class="math display">\[\begin{eqnarray}
    \gamma(0) &amp; = &amp; a_1 \gamma(1) + a_2 \gamma(2) + \ldots + a_p \gamma(p) + \sigma^2 \nonumber \\
    \gamma(1) &amp; = &amp; a_1 \gamma(0) + a_2 \gamma(1) + \ldots + a_p \gamma(p-1) \nonumber \\
    \vdots \nonumber \\
    \gamma(p) &amp; = &amp; a_1 \gamma(p-1) + a_2 \gamma(p-2) + \ldots + a_p \gamma(0) \nonumber
\end{eqnarray}\]</span></p>
<p>De esta forma, es fácil observar que la ecuación general para <span class="math inline">\(p &gt; 0\)</span> estará dada por:
<span class="math display">\[\begin{equation}
    \gamma(p) - a_1 \gamma(\tau - 1) - a_2 \gamma(\tau - 2) - \ldots - a_p \gamma(\tau - p) = 0
    \label{Gamma_p}
\end{equation}\]</span></p>
<p>Dividiendo la ecuación () por <span class="math inline">\(\gamma(0)\)</span>, se obtiene la siguiente ecuación:
<span class="math display">\[\begin{equation}
    \rho(p) - a_1 \rho(\tau - 1) + a_2 \rho(\tau - 2) + \ldots + a_p \rho(\tau - p) = 0
\end{equation}\]</span></p>
<p>Así, podemos escribir el siguiente sistema de ecuaciones:
<span class="math display">\[\begin{eqnarray}
    \rho(1) &amp; = &amp; a_1 + a_2 \rho(1) + a_3 \rho(2) + \ldots + a_p \rho(p-1) \nonumber \\
    \rho(2) &amp; = &amp; a_1 \rho(1) + a_2 + a_3 \rho(1) + \ldots + a_p \rho(p-2) \nonumber \\
    &amp; \vdots &amp; \nonumber \\
    \rho(p) &amp; = &amp; a_1 \rho(p-1) + a_2 \rho(p-2) + \ldots + a_p \nonumber
\end{eqnarray}\]</span></p>
<p>Lo anterior se puede expresar como un conjunto de vectores y matrices de la siguiente forma:
<span class="math display">\[\begin{equation}
    \left[ 
    \begin{array}{c}
        \rho(1) \\
        \rho(2) \\
        \vdots \\
        \rho(p)
    \end{array} 
    \right]
    = 
    \left[ 
    \begin{array}{c c c c}
        1 &amp; \rho(1) &amp; \ldots &amp; \rho(p - 1) \\
        \rho(1) &amp; 1 &amp; \ldots &amp; \rho(p - 2) \\
        \rho(2) &amp; \rho(1) &amp; \ldots &amp; \rho(p - 3) \\
        \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
        \rho(p - 1) &amp; \rho(p - 2) &amp; \ldots &amp; 1 \\
    \end{array} 
    \right]
    \left[ 
    \begin{array}{c}
        a_1 \\
        a_2 \\
        a_3 \\
        \vdots \\
        a_p \\
    \end{array} 
    \right]
\end{equation}\]</span></p>
<p>De lo anterior podemos escribir la siguiente ecuación que es una forma alternativa para expresar los valores de los coefientes <span class="math inline">\(a_i\)</span> de la la solución del proceso <span class="math inline">\(AR(p)\)</span>:
<span class="math display">\[\begin{equation}
    \mathbf{\rho} = \mathbf{R} \mathbf{a}
\end{equation}\]</span></p>
<p>Es decir, podemos obtener la siguiente expresión:
<span class="math display">\[\begin{equation}
    \mathbf{a} = \mathbf{R}^{-1} \mathbf{\rho}
\end{equation}\]</span></p>
<p>Ahora veámos un ejemplo. Utilizaremos la serie de Pasajeros en vuelos internacionales de salida para estimar un <span class="math inline">\(AR(p)\)</span> mediante el método de máxima verosimilitud (ML). Antes de realizar el proceso de estimación consideremos una transformación de la serie en logaritmos y una más en diferencias logaritmicas; lo anterior con el objeto de obtener un conjunto de series de tiempo suavizada y expresada en tasas de crecimiento, con un comportamiento parecido a un proceso estacionario.</p>
Primero, para decidir si se realizará un <span class="math inline">\(AR(p)\)</span> para la serie en niveles o en diferencias análizaremos su gráfica. La serie de e Pasajeros en vuelos internacionales de salidas se muestra en la Figura . En está se muestra la gráfica de la serie en niveles (sin transformación logaritmica y con transformación logaritmica) y en diferencias logaritmicas mensuales (es decir, con diferencia respecto del mes inmediato anterior).

De la gráfica en la Figura  observamos que quizá le mejor forma de estimar un AR(p) es mediante la serie en diferencias, ya que ésta es la que parece ser una serie estacionaria. A continuación, estimaremos una AR(4) para la serie en diferencias logarimitcas (<span class="math inline">\(DLPaxInt_t\)</span>):


Entre parentésis indicamos los errores estándar y reportamos el estadístico de Akaike, AIC. Finalmente, podemos determinar si las soluciones serán convergentes, para ello en la Figura  mostramos las raíces asociadas a cada uno de los polinomios. De la inspección visual podemos concluir que el AR(4) representan una solución convergente y estable.

</div>
</div>
<div id="procesos-de-medias-móviles-ma" class="section level2">
<h2><span class="header-section-number">4.2</span> Procesos de Medias Móviles (MA)</h2>
<div id="ma1" class="section level3">
<h3><span class="header-section-number">4.2.1</span> MA(1)</h3>
<p>Una vez planteado el proceso generalizado de <span class="math inline">\(AR(p)\)</span>, iniciamos el planteamiento de los proceso de medias móviles, denotados como <span class="math inline">\(MA(q)\)</span>. Iniciemos con el planteamiento del proceso <span class="math inline">\(MA(1)\)</span>, que se puede escribir como una ecuación como la siguiente:
<span class="math display">\[\begin{equation}
    X_t = \mu + U_t - b_1 U_{t-1}
    \label{MA1_Eq}
\end{equation}\]</span></p>
<p>O como:
<span class="math display">\[\begin{equation}
    X_t - \mu = (1 - b_1 L) U_{t}
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(U_t\)</span> es un proceso puramente aleatorio, es decir, con <span class="math inline">\(\mathbb{E}[U_t] = 0\)</span>, <span class="math inline">\(Var[U_t] = \sigma^2\)</span>, y <span class="math inline">\(Cov[U_t, U_s] = 0\)</span>. Así, un proceso <span class="math inline">\(MA(1)\)</span> puede verse como un proceso AR con una descomposición de Wold en la que <span class="math inline">\(\psi_0 = 1\)</span>, <span class="math inline">\(\psi_1 = - b_1\)</span> y <span class="math inline">\(\psi_j = 0\)</span> para todo <span class="math inline">\(j &gt; 1\)</span>.</p>
<p>Al igual que los procesos autoregresivos, determinaremos los momentos de un proceso <span class="math inline">\(MA(1)\)</span>. En el caso de la media observamos que será:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}[X_t] &amp; = &amp; \mu + \mathbb{E}[U_t] - b_1 \mathbb{E}[U_{t - 1}] \nonumber \\
    &amp; = &amp; \mu
\end{eqnarray}\]</span></p>
<p>Por su parte la varianza estará dada por:
<span class="math display">\[\begin{eqnarray}
    Var[X_t] &amp; = &amp; \mathbb{E}[(X_t - \mu)^2] \nonumber \\
    &amp; = &amp; \mathbb{E}[(U_t - b_1 U_{t-1})^2] \nonumber \\
    &amp; = &amp; \mathbb{E}[U_t^2 - 2 b_1 U_t U_{t-1} + b_1^2 U_{t - 1}^2] \nonumber \\
    &amp; = &amp;\mathbb{E}[U_t^2] - 2 b_1 \mathbb{E}[U_t U_{t-1}] + b_1^2 \mathbb{E}[U_{t - 1}^2]] \nonumber \\
    &amp; = &amp; \sigma^2 + b_1^2 \sigma^2 \nonumber \\
    &amp; = &amp; (1 + b_1^2) \sigma^2 = \gamma(0)
\end{eqnarray}\]</span></p>
<p>De esta forma, la varianza del proceso es constante en cualquier periodo <span class="math inline">\(t\)</span>. Para determinar la covarianza utilizaremos la siguiente ecuación:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}[(x_t - \mu)(x_{t + \tau} - \mu)] &amp; = &amp; \mathbb{E}[(U_t - b_1 U_{t-1})(U_{t + \tau} - b_1 U_{t + \tau - 1})] \nonumber \\
    &amp; = &amp; \mathbb{E}[U_t U_{t + \tau} - b_1 U_t U_{t + \tau - 1} - b_1 U_{t - 1} U_{t + \tau} \nonumber \\
    &amp;   &amp; + b_1^2 U_{t - 1} U_{t + \tau - 1}] \nonumber \\
    &amp; = &amp; \mathbb{E}[U_t U_{t + \tau}] - b_1 \mathbb{E}[U_t U_{t + \tau - 1}] \nonumber \\
    &amp;   &amp; - b_1 \mathbb{E}[U_{t - 1} U_{t + \tau}] + b_1^2 \mathbb{E}[U_{t - 1} U_{t + \tau - 1}]
    \label{MA1_Cov}
\end{eqnarray}\]</span></p>
<p>Si hacemos sustituciones de diferentes valores de <span class="math inline">\(\tau\)</span> en la ecuación () notaremos que la covarianza será distinta de cero únicamente para el caso de <span class="math inline">\(\tau = 1, -1\)</span>. En ambos casos tendremos como resultado:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}[(x_t - \mu)(x_{t + 1} - \mu)] &amp; = &amp; \mathbb{E}[(x_t - \mu)(x_{t - 1} - \mu)] \nonumber \\
    &amp; = &amp; - b_1 \mathbb{E}[U_t U_{t}] \nonumber \\
    &amp; = &amp; - b_1 \mathbb{E}[U_{t - 1} U_{t - 1}] \nonumber \\ 
    &amp; = &amp; - b_1^2 \sigma^2 = \gamma(1)
\end{eqnarray}\]</span></p>
<p>De esta forma tendremos que las funciones de autocorrelación estarán dadas por los siguientes casos:
<span class="math display">\[\begin{eqnarray}
    \rho(0) &amp; = &amp; 1 \nonumber \\
    \rho(1) &amp; = &amp; \frac{- b_1}{1 + b_1^2} \nonumber \\
    \rho(\tau) &amp; = &amp; 0 \text{ para todo } \tau &gt; 1 \nonumber 
\end{eqnarray}\]</span></p>
<p>Ahora regresando a la ecuación (), su solución la podemos expresar como:
<span class="math display">\[\begin{eqnarray}
    U_ t &amp; = &amp; - \frac{\mu}{1 - b_1} + \frac{1}{1 - b_1 L} X_t \nonumber \\
    &amp; = &amp; - \frac{\mu}{1 - b_1} + X_t + b_1 X_{t-1} + b_1^2 X_{t-2} + \ldots \nonumber
\end{eqnarray}\]</span></p>
<p>Donde la condición para que se cumpla esta ecuación es que <span class="math inline">\(\abs{b_1} &lt; 1\)</span>. La manera de interpretar esta condición es como una condición de estabilidad de la solución y cómo una condición de invertibilidad. Notemos que un <span class="math inline">\(MA(1)\)</span> (y en general un <span class="math inline">\(MA(q)\)</span>) es equivalente a un <span class="math inline">\(AR(\infty)\)</span>, es decir, cuando se invierte un MA se genera un AR con infinitos rezagos.</p>
<p>En esta sección no desarrollaremos un ejemplo, primero explicaremos en qué consiste una modelación del tipo <span class="math inline">\(MA(q)\)</span> y después platearemos un ejemplo en concreto.</p>
</div>
<div id="maq" class="section level3">
<h3><span class="header-section-number">4.2.2</span> MA(q)</h3>
<p>En general, el proceso de medias móviles de orden <span class="math inline">\(q\)</span>, <span class="math inline">\(MA(q)\)</span>, puede ser escrito como:
<span class="math display">\[\begin{equation}
    X_t = \mu + U_t - b_1 U_{t-1} - b_2 U_{t-2} - \ldots - b_q U_{t-q}
    \label{MAq_EQ}
\end{equation}\]</span></p>
<p>Podemos reescribir la ecuación () utilizando el operador rezago, así tendrémos el proceso de <span class="math inline">\(MA(q)\)</span> como:
<span class="math display">\[\begin{eqnarray}
    X_t - \mu &amp; = &amp; (1 - b_1 L - b_2 L^2 - \ldots - b_q L^q) U_{t} \nonumber \\
    X_t - \mu &amp; = &amp; \beta(L) U_t
    \label{MAq_Red}
\end{eqnarray}\]</span></p>
<p>Donde <span class="math inline">\(U_t\)</span> es un proceso puramente aleatorio con <span class="math inline">\(\mathbb{E}[U_t] = 0\)</span>, <span class="math inline">\(Var[U_t] = \mathbb{E}[U_t^2] = 0\)</span> y <span class="math inline">\(Cov[U_t, U_s] = \mathbb{E}[U_t, U_s] = 0\)</span>, y <span class="math inline">\(\beta(L) = 1 - b_1 L - b_2 L^2 - \ldots - b_q L^q\)</span> es un polinomio del operador rezago <span class="math inline">\(L\)</span>. la ecuación () puede ser interpretada como un proceso <span class="math inline">\(AR(q)\)</span> sobre la serie <span class="math inline">\(U_t\)</span>.</p>
<p>Ahora determinemos los momentos de un proceso <span class="math inline">\(MA(q)\)</span>:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}[X_t] &amp; = &amp; \mathbb{E}[\mu + U_t - b_1 U_{t-1} - b_2 U_{t-2} - \ldots - b_q U_{t-q}] \nonumber \\
    &amp; = &amp; \mu + \mathbb{E}[U_t] - b_1 \mathbb{E}[U_{t-1}] - b_2 \mathbb{E}[U_{t-2}] - \ldots - b_q \mathbb{E}[U_{t-q}] \nonumber \\
    &amp; = &amp; \mu
\end{eqnarray}\]</span></p>
<p>En el caso de la varianza tenemos que se puede expresar como:
<span class="math display">\[\begin{eqnarray}
    Var[X_t] &amp; = &amp; \mathbb{E}[(X_t - \mu)^2] \nonumber \\
    &amp; = &amp; \mathbb{E}[(U_t - b_1 U_{t-1} - b_2 U_{t-2} - \ldots - b_q U_{t-q})^2] \nonumber \\
    &amp; = &amp; \mathbb{E}[U_t^2 + b_1^2 U_{t-1}^2 + b_2^2 U_{t-2}^2 + \ldots + b_q^2 U_{t-q}^2 \nonumber \\
    &amp;   &amp; - 2 b_1 U_t U_{t - 1} - \ldots - 2 b_{q - 1} b_q U_{t - q + 1} U_{t - q}] \nonumber \\
    &amp; = &amp; \mathbb{E}[U_t^2] + b_1^2 \mathbb{E}[U_{t-1}^2] + b_2^2 \mathbb{E}[U_{t-2}^2] + \ldots + b_q^2 \mathbb{E}[U_{t-q}^2] \nonumber \\
    &amp;   &amp; - 2 b_1 \mathbb{E}[U_t U_{t - 1}] - \ldots - 2 b_{q - 1} b_q \mathbb{E}[U_{t - q + 1} U_{t - q}] \nonumber \\
    &amp; = &amp; \sigma^2 + b^2_1 \sigma^2 + b^2_2 \sigma^2 + \ldots + b^2_q \sigma^2 \nonumber \\
    &amp; = &amp; (1 + b^2_1 + b^2_2 + \ldots + b^2_q) \sigma^2
\end{eqnarray}\]</span></p>
<p>En el caso de las covarianzas podemos utilizar una idea similar al caso del <span class="math inline">\(AR(p)\)</span>, construir una expresión general para cualquier rezago <span class="math inline">\(\tau\)</span>:
<span class="math display">\[\begin{eqnarray}
    Cov[X_t, X_{t + \tau}] &amp; = &amp; \mathbb{E}[(X_t - \mu)(X_{t + \tau} - \mu)] \nonumber \\
    &amp; = &amp; \mathbb{E}[(U_t - b_1 U_{t-1} - b_2 U_{t-2} - \ldots - b_q U_{t-q}) \nonumber \\
    &amp;   &amp; (U_{t + \tau} - b_1 U_{t + \tau -1} - b_2 U_{t + \tau -2} - \ldots - b_q U_{t + \tau - q})] \nonumber
\end{eqnarray}\]</span></p>
<p>La expresión anterior se puede desarrollar para múltiples casos de <span class="math inline">\(\tau = 1, 2, \ldots, q\)</span>. De esta forma tenemos el siguiente sistema:
<span class="math display">\[\begin{eqnarray}
    \tau = 1 &amp; : &amp; \gamma(1) = (- b_1 + b_1 b_2 + \ldots + b_{q-1} b_q) \sigma^2 \nonumber \\
    \tau = 2 &amp; : &amp; \gamma(2) = (- b_2 + b_1 b_3 + \ldots + b_{q-2} b_q) \sigma^2 \nonumber \\
    &amp; \vdots &amp; \nonumber \\
    \tau = q &amp; : &amp; \gamma(q) = b_q \sigma^2 \nonumber
\end{eqnarray}\]</span></p>
<p>Donde <span class="math inline">\(\gamma(\tau) = 0\)</span> para todo <span class="math inline">\(\tau &gt; q\)</span>. Es decir, todas las autocovarianzas y autocorrelaciones con ordenes superiores a <span class="math inline">\(q\)</span> son cero (0). De esta forma, esta caracterítica teórica permite identificar el orden de <span class="math inline">\(MA(q)\)</span> visualizando la función de autocorrelación y verificando a partir de cual valor de rezago la autocorrelación es no significaiva.</p>
<p>Regresando al problema original que es el de determinar una solución para la eucación (), tenemos que dicha solución estará dada por un <span class="math inline">\(AR(\infty)\)</span> en términos de <span class="math inline">\(U_t\)</span>:
<span class="math display">\[\begin{eqnarray}
    U_t &amp; = &amp; - \frac{\mu}{1 - b_1 - b_2 - \ldots - b_q} + \beta(L)^{-1} X_t \nonumber \\
    &amp;   &amp; - \frac{\mu}{1 - b_1 - b_2 - \ldots - b_q} + \sum_{j = 0}^{\infty} c_j X_{t-j} 
    \label{MAq_Eq_Sol}
\end{eqnarray}\]</span></p>
<p>Donde se cumple que: <span class="math inline">\(1 = (1 - b_1 L^1 - b_2 L^2 - \ldots - b_q L^q)(1 - c_1 L - c_2 L^2 - \ldots)\)</span> y los coeficientes <span class="math inline">\(c_j\)</span> se pueden determinar por un método de coeficientes indeterminados y en términos de los valores <span class="math inline">\(b_i\)</span>. De igual forma que en el caso de la ecuación (), en la ecuación () se deben cumplir condiciones de estabilidad asociadas con las raíces del polinomio carácterististico dado por:
<span class="math display">\[\begin{equation}
    1 - b_1 x - b_2 x^2 - \ldots b_q x^q = 0
\end{equation}\]</span></p>
<p>El cual debe cumplir que <span class="math inline">\(\abs{x_i} &lt; 1\)</span> y que <span class="math inline">\(1 - b_1 - b_2 - \ldots b_q &lt; 1\)</span>.</p>
<p>Ahora veamos un ejemplo del proceso <span class="math inline">\(MA(q)\)</span>, para lo cual retomaremos la serie de Pasajeros transportados en el metro de la CDMX (<span class="math inline">\(PaxMetro\)</span>). Estimaremos el <span class="math inline">\(MA(q)\)</span> mediante el método de máxima verosimilitud (ML). Antes de realizar el proceso de estimación consideremos una transformación de la serie en logaritmos y una más en diferencias logaritmicas; lo anterior con el objeto de obtener un conjunto de series de tiempo suavizada y expresada en tasas de crecimiento, con un comportamiento parecido a un proceso estacionario.</p>
<p>La serie de Pasajeros transportados en el metro de la CDMX se muestra en la Figura  se muestra la gráfica de la serie en niveles (sin transformación logarítmica y con transformación logarítmica) y en diferencias logarítmicas mensuales (es decir, con una diferencia respecto del mes inmediato anterior). Utilizaremos la serie en diferencias, ya que es la que parece ser estacionaria. Esta serie tiene la peculiaridad de que tiene un salto a la baja y uno al alza entre septiembre de 2017 y octubre de 2017. Para controlar ese efecto, en nuestro modelo <span class="math inline">\(MA(q)\)</span> incluiremos dos variables dummies para dichos meses.</p>
A continuación, estimaremos una <span class="math inline">\(MA(4)\)</span> para la serie en diferencias:


Entre parentésis indicamos los errores estándar y al final reportamos el estadístico de Akaike, AIC. Finalmente, podemos determinar si la solución serán convergente, para ello en la Figura  mostramos las raíces asociadas a cada uno de los polinomios. De la inspección visual podemos concluir que ambas propuesta de AR(2) representan una solución convergente y estable.

</div>
</div>
<div id="procesos-armap-q-y-arimap-d-q" class="section level2">
<h2><span class="header-section-number">4.3</span> Procesos ARMA(p, q) y ARIMA(p, d, q)</h2>
<p>Hemos establecido algunas relaciones las de los porcesos AR y los procesos MA, es decir, cómo un <span class="math inline">\(MA(q)\)</span> de la serie <span class="math inline">\(X_t\)</span> puede ser reexpresada como un <span class="math inline">\(AR(\infty)\)</span> de la serie <span class="math inline">\(U_t\)</span>, y viceversa un <span class="math inline">\(AR(p)\)</span> de la serie <span class="math inline">\(X_t\)</span> puede ser reeexpresada como un <span class="math inline">\(MA(\infty)\)</span>.</p>
<p>En este sentido, para cerrar esta sección veámos el caso de la especificación que conjunta ambos modelos en un modelo general conocido como <span class="math inline">\(ARMA(p, q)\)</span> o <span class="math inline">\(ARIMA(p, d, q)\)</span>. La diferencia entre el primero y el segundo es las veces que su tuvo que diferenciar la serie analizada, registro que se lleva en el índice <span class="math inline">\(d\)</span> de los paramétros dentro del concepto <span class="math inline">\(ARIMA(p, d, q)\)</span>. No obstante, en general nos referiremos al modelo como <span class="math inline">\(ARMA(p, q)\)</span> y dependerá del analista si modela la serie en niveles (por ejemplo, en logaritmos) o en diferencias logarítmicas (o diferencias sin logaritmos).</p>
<div id="arma1-1" class="section level3">
<h3><span class="header-section-number">4.3.1</span> ARMA(1, 1)</h3>
<p>Dicho lo anterior vamos a empezar con el análisis de un <span class="math inline">\(ARMA(1, 1)\)</span>. Un proceso <span class="math inline">\(ARMA(1, 1)\)</span> puede verse como:
<span class="math display">\[\begin{equation}
    X_t = \delta + a_1 X_{t - 1} + U_t - b_1 U_{t - 1}
    \label{ARMA11_Eq}
\end{equation}\]</span></p>
<p>Aplicando el operado rezago podemos rescribir la ecuación () como:
<span class="math display">\[\begin{equation}
    (1 - a_1 L) X_t = \delta + (1 - b_1 L) U_t
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(U_t\)</span> es un proceso pueramente aleatorio como en los casos de <span class="math inline">\(AR(p)\)</span> y <span class="math inline">\(MA(q)\)</span>, y <span class="math inline">\(X_t\)</span> puede ser una serie en niveles o en diferencias (ambas, en términos logarítmicos).</p>
<p>Así, el modelo <span class="math inline">\(ARIMA (p, q)\)</span> también tiene una representación de Wold que estará dada por las siguientes expresiones:
<span class="math display">\[\begin{equation}
    X_t = \frac{\delta}{1 - a_1} + \frac{1 - b_1 L}{1 - a_1 L} U_t
    \label{ARMA11_Prev}
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(a_1 \neq b_1\)</span>, puesto que en caso contrario <span class="math inline">\(X_t\)</span> sería un proceso puramente aleatorio con una media <span class="math inline">\(\mu = \frac{\delta}{1 - a_1}\)</span>. Así, podemos reescribir la descomposición de Wold a partir del componente de la ecuación ():
<span class="math display">\[\begin{equation}
    \frac{1 - b_1 L}{1 - a_1 L} = \psi_0 + \psi_1 L + \psi_2 L^2 + \psi_3 L^3 + \ldots 
    \label{ARMA11_EQ_Wold}
\end{equation}\]</span></p>
<p>Está ecuación es equivalente a la expresión:
<span class="math display">\[\begin{eqnarray}
    (1 - b_1 L) &amp; = &amp; (1 - a_1 L)(\psi_0 + \psi_1 L + \psi_2 L^2 + \psi_3 L^3 + \ldots) \nonumber \\
    &amp; = &amp; \psi_0 + \psi_1 L + \psi_2 L^2 + \psi_3 L^3 + \ldots \nonumber \\
    &amp;   &amp; - a_1 \psi_0 L - a_1 \psi_1 L^2 - a_2 \psi_2 L^3 - a_1 \psi_3 L^4 - \ldots \nonumber
\end{eqnarray}\]</span></p>
De esta forma podemos establecer el siguiente sistema de coeficientes indeterminados:

<p>Así, la solución a la ecuación () estará dada por la siguiente generalización:
<span class="math display">\[\begin{equation}
    X_t = \frac{\delta}{1 - a_1} + U_t + (a_1 - b_1) U_{t - 1} + a_1(a_1 - b_1) U_{t - 2} + a_1^2(a_1 - b_1) U_{t - 3} + \ldots
    \label{ARMA11_Sol}
\end{equation}\]</span></p>
<p>En la ecuación () las condiciones de estabilidad y de invertibilidad del sistema (de un MA a un AR, y viceversa) estarán dadas por: <span class="math inline">\(\abs{a_1} &lt; 1\)</span> y <span class="math inline">\(\abs{b_1} &lt; 1\)</span>. Adicionalmente, la ecuación () expresa cómo una serie que tiene un comportamiento <span class="math inline">\(ARMA(1, 1)\)</span> es equivalente a una serie modelada bajo un <span class="math inline">\(MA(\infty)\)</span>.</p>
<p>Al igual que en los demás modelos, ahora determinaremos los momentos del proceso <span class="math inline">\(ARMA(1, 1)\)</span>. La media estará dada por:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}[X_t] &amp; = &amp; \mathbb{E}[\delta + a_1 X_{t-1} + U_t - b_1 U_{t-1}] \nonumber \\
    &amp; = &amp; \delta + a_1 \mathbb{E}[X_{t-1}] \nonumber \\
    &amp; = &amp; \frac{\delta}{1 - a_1} \nonumber \\
    &amp; = &amp; \mu
\end{eqnarray}\]</span></p>
<p>Donde hemos utilizado que <span class="math inline">\(\mathbb{E}[X_t] = \mathbb{E}[X_{t-1}] = \mu\)</span>. Es decir, la media de un <span class="math inline">\(ARMA(1, 1)\)</span> es idéntica a la de un <span class="math inline">\(AR(1)\)</span>.</p>
<p>Para determinar la varianza tomaremos una estrategía similar a los casos de <span class="math inline">\(AR(p)\)</span> y <span class="math inline">\(MA(q)\)</span>. Por lo que para todo <span class="math inline">\(\tau \geq 0\)</span>, y suponiendo por simplicidad que <span class="math inline">\(\delta = 0\)</span> (lo que implica que <span class="math inline">\(\mu = 0\)</span>) tendremos:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}[X_{t-\tau} X_t] &amp; = &amp; \mathbb{E}[(X_{t-\tau}) \cdot (a_1 X_{t-1} + U_t - b_1 U_{t-1})] \nonumber \\
    &amp; = &amp; a_1 \mathbb{E}[X_{t-\tau} X_{t-1}] + \mathbb{E}[X_{t-\tau} U_t] - b_1 \mathbb{E}[X_{t-\tau} U_{t-1}]
    \label{ARMA11_Cov}
\end{eqnarray}\]</span></p>
<p>De la ecuación () podemos determinar una expresión para el caso de <span class="math inline">\(\tau = 0\)</span>:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}[X_{t} X_t] &amp; = &amp; \gamma(0) \nonumber \\
    &amp; = &amp; a_1 \gamma(1) + \mathbb{E}[U_t X_t] - b_1 \mathbb{E}[X_t U_{t-1}] \nonumber \\
    &amp; = &amp; a_1 \gamma(1) + \sigma^2 + b_1 \mathbb{E}[U_{t-1} (a_1 X_{t-1} + U_t - b_1 U_{t-1})] \nonumber \\
    &amp; = &amp; a_1 \gamma(1) + \sigma^2 - b_1 a_1 \sigma^2 + b_1 \sigma^2 \nonumber \\
    &amp; = &amp; a_1 \gamma(1) + (1 - b_1 (a_1 - b_1)) \sigma^2
\end{eqnarray}\]</span></p>
<p>Para el caso en que <span class="math inline">\(\tau = 1\)</span>:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}[X_{t-1} X_t] &amp; = &amp; \gamma(1) \nonumber \\
    &amp; = &amp; a_1 \gamma(0) + \mathbb{E}[X_{t-1} U_t] - b_1 \mathbb{E}[X_{t-1} U_{t-1}] \nonumber \\
    &amp; = &amp; a_1 \gamma(0) - b_1 \sigma^2
\end{eqnarray}\]</span></p>
<p>Estas últimas expresiones podemos resolverlas como sistema para determinar los siguientes valores:
<span class="math display">\[\begin{eqnarray}
    \gamma(0) &amp; = &amp; \frac{1 + b_1^2 - 2 a_1 b_1}{1 - a_1^2} \sigma^2 \\
    \gamma(1) &amp; = &amp; \frac{(a_1 - b_1)(1 - a_1 b_1)}{1 - a_1^2} \sigma^2
\end{eqnarray}\]</span></p>
<p>En general para cualquier valor <span class="math inline">\(\tau \geq 2\)</span> tenemos que la autocovarianza y la función de autocorrelación serán:
<span class="math display">\[\begin{eqnarray}
    \gamma(\tau) = a_1 \gamma(\tau - 1) \\
    \rho(\tau) = a_1 \rho(\tau - 1)
\end{eqnarray}\]</span></p>
<p>Por ejemplo, para el caso de <span class="math inline">\(\tau = 1\)</span> tendríamos:
<span class="math display">\[\begin{equation}
    \rho(1) = \frac{(a_1 - b_1)(1 - a_1 b_1)}{1 + b_1^2 - 2 a_1 b_1}
\end{equation}\]</span></p>
<p>De esta forma, la función de autocorrelación oscilará en razón de los valores que tome <span class="math inline">\(a_1\)</span> y <span class="math inline">\(b_1\)</span>.</p>
</div>
<div id="armap-q" class="section level3">
<h3><span class="header-section-number">4.3.2</span> ARMA(p, q)</h3>
<p>La especificación general de un <span class="math inline">\(ARMA(p, q)\)</span> (donde <span class="math inline">\(p, q \in \mathbb{N}\)</span>) puede ser descrita por la siguiente ecuación:
<span class="math display">\[\begin{eqnarray}
    X_t &amp; = &amp; \delta + a_1 X_{t - 1} + a_2 X_{t - 2} + \ldots + a_p X_{t - p} \nonumber \\
    &amp;   &amp; + U_t - b_1 U_{t - 1} - b_2  U_{t - 2} - \ldots - b_q  U_{t - q}
    \label{ARMApq_Eq}
\end{eqnarray}\]</span></p>
<p>Donde <span class="math inline">\(U_t\)</span> es un proceso puramente aleatorio, y <span class="math inline">\(X_t\)</span> puede ser modelada en niveles o en diferencias (ya sea en logaritmos o sin transformación logarítmica).</p>
<p>Mediante el uso del operador rezago se puede escribir la ecuación () como:
<span class="math display">\[\begin{equation}
    (1 - a_1 L - a_2 L^2 - \ldots - a_p L^p) X_t = \delta + (1 - b_1 L - b_2 L^2 - \ldots - b_q L^q) U_t 
    \label{ARMApq_EQLag}
\end{equation}\]</span></p>
<p>En la ecuación () definamos dos polinomios: <span class="math inline">\(\alpha(L) = (1 - a_1 L - a_2 L^2 - \ldots - a_p L^p)\)</span> y <span class="math inline">\(\beta(L) = (1 - b_1 L - b_2 L^2 - \ldots - b_q L^q)\)</span>. Así, podemos reescribir la ecuación () como:
<span class="math display">\[\begin{equation}
    \alpha(L) X_t = \delta + \beta(L) U_t 
\end{equation}\]</span></p>
<p>Asumiendo que existe el polinomio inverso tal que: <span class="math inline">\(\alpha(L)^{-1}\alpha(L) = 1\)</span>.La solución entonces puede ser escrita como:
<span class="math display">\[\begin{eqnarray}
    X_t &amp; = &amp; \alpha(L)^{-1} \delta + \alpha(L)^{-1} \beta(L) U_t \nonumber \\
    &amp; = &amp; \frac{\delta}{1 - a_1 - a_2 - \ldots - a_p} + \frac{\beta(L)}{\alpha(L)} U_t \nonumber \\
    &amp; = &amp; \frac{\delta}{1 - a_1 - a_2 - \ldots - a_p} + U_t + \psi_1 L U_t + \psi_2 L^2 U_t + \ldots
    \label{ARMApq_Wold}
\end{eqnarray}\]</span></p>
<p>Donde la ecuación () nos permite interpretar que un ARMA(p, q) se puede reexpresar e interpreetar como un <span class="math inline">\(MA(\infty)\)</span> y donde las condiciones para la estabilidad de la solución y la invertibilidad es que las ráices de los polinomios característicos <span class="math inline">\(\alpha(L)\)</span> y <span class="math inline">\(\beta(L)\)</span> son en valor absoluto menores a 1.</p>
<p>Adicionalmente, la fracción en la ecuación () se puede descomponer como en la forma de Wold:
<span class="math display">\[\begin{equation}
    \frac{\beta(L)}{\alpha(L)} = 1 + \psi_1 L + \psi_2 L^2 + \ldots
\end{equation}\]</span></p>
<p>Bajo los supuestos de estacionariedad del componente <span class="math inline">\(U_t\)</span>, los valores de la media y varianza de un proceso <span class="math inline">\(ARMA(p, q)\)</span> serán como describimos ahora. Para el caso de la media podemos partir de la ecuación () para generar:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}[X_t] &amp; = &amp; \mathbb{E}\left[ \frac{\delta}{1 - a_1 - a_2 - \ldots - a_p} + U_t + \psi_1 U_{t-1} + \psi_2 U_{t-2} + \ldots \right] \nonumber \\
    &amp; = &amp; \frac{\delta}{1 - a_1 - a_2 - \ldots - a_p} \nonumber \\
    &amp; = &amp; \mu
\end{eqnarray}\]</span></p>
<p>Esta expresión indica que en general un proceso <span class="math inline">\(ARMA(p, q)\)</span> converge a una media idéntica a la de un porceso <span class="math inline">\(AR(p)\)</span>. Para determinar la varianza utilizaremos la misma estratégia que hemos utilizado para otros modelos <span class="math inline">\(AR(p)\)</span> y <span class="math inline">\(MA(q)\)</span>.</p>
<p>Sin pérdida de generalidad podemos asumir que <span class="math inline">\(\delta = 0\)</span>, lo que implica que <span class="math inline">\(\mu = 0\)</span>, de lo que podemos establecer una expresión de autocovarianzas para cualquier valor <span class="math inline">\(\tau = 0, 1, 2, \ldots\)</span>:
<span class="math display">\[\begin{eqnarray}
    \gamma(\tau) &amp; = &amp; \mathbb{E}[X_{t-\tau} X_t] \nonumber \\
    &amp; = &amp; \mathbb{E}[X_{t-\tau} (\delta + a_1 X_{t - 1} + a_2 X_{t - 2} + \ldots + a_p X_{t - p} \nonumber \\
    &amp;   &amp; + U_t - b_1 U_{t - 1} - b_2  U_{t - 2} - \ldots - b_q  U_{t - q})] \nonumber \\
    &amp; = &amp; a_1 \gamma(\tau - 1) + a_2 \gamma(\tau - 2) + \ldots + a_p \gamma(\tau - p) \nonumber \\
    &amp;   &amp; + \mathbb{E}[X_{t-\tau} U_{t}] - b_1  \mathbb{E}[X_{t-\tau} U_{t-1}] - \ldots  - b_q  \mathbb{E}[X_{t-\tau} U_{t-q}] 
\end{eqnarray}\]</span></p>
<p>Ahora veámos un ejemplo. Utilizaremos la serie de Pasajeros en vuelos nacionales de salida para estimar un <span class="math inline">\(ARMA(p, q)\)</span> mediante el método de máxima verosimilitud (ML). Antes de realizar el proceso de estimación consideremos una transformación de la serie en diferencias logaritmicas, ya que según la gráfica en la Figura () esa es la que puede ser estacionaria.</p>
<p>A continuación, estimaremos una <span class="math inline">\(ARMA(1, 1)\)</span> para la serie en diferencias logarimitcas (<span class="math inline">\(DLPaxNal_t\)</span>). También incorporaremos al análisis variables exogénas tales como dummies de estacionalidad. En particular, utilizaremos los meses de enero, febrero, julio y diciembre. No debe pasar desapercibido que un análisis de estacionalidad más formal debeería considerar todos los meses para separar del término de error la parte que puedee ser explicada por los ciclos estacionales.</p>
Así obtenemos el siguiente resultado:


Donde entre parentésis indicamos los errores estándar. Adicionalmente, reportamos el estadístico de Akaike (AIC). Finalmente, podemos determinar si las soluciones serán convergentes, para ello en la Figura  mostramos las raíces asociadas a cada uno de los polinomios. De la inspección visual podemos concluir que tenemos una solución convergente y estable. Por su parte la Figura () muestra los residuales de la estimación del <span class="math inline">\(ARMA(1, 1)\)</span>.


<p>En lo que resta de este capítulo, utilizaremos la serie en diferencias logarítmicas de los pasajeros en vuelos nacionales de salida, <span class="math inline">\(DLPaxNal_t\)</span>, para discutir los ejemplos que ilustran cada uno de los puntos teóricos que a continuación exponemos.</p>
</div>
</div>
<div id="función-de-autocorrelación-parcial" class="section level2">
<h2><span class="header-section-number">4.4</span> Función de Autocorrelación Parcial</h2>
<p>Ahora introduciremos el concepto de Función de Autocorrelación Parcial (PACF, por sus siglas en inglés). Primero, dadas las condiciones de estabilidad y de convergencia, si suponemos que un proceso AR, MA, ARMA o ARIMA tienen toda la información de los rezagos de la serie en conjunto y toda la información de los promedio móviles del término de error, resulta importante construir una métrica para distinguir el efecto de <span class="math inline">\(X_{t - \tau}\)</span> o el efecto de <span class="math inline">\(U_{t - \tau}\)</span> (para cualquier <span class="math inline">\(\tau\)</span>) sobre <span class="math inline">\(X_t\)</span> de forma individual.</p>
<p>La idea es construir una métrica de la correlación que existe entre las diferentes varibles aleatorias, si para tal efecto se ha controlado el efecto del resto de la información. Así, podemos definir la ecuación que puede responder a este planteamiento como:
<span class="math display">\[\begin{equation}
    X_t = \phi_{k1} X_{t-1} + \phi_{k2} X_{t-2} + \ldots + \phi_{kk} X_{t-k} + U_t
    \label{PACF_Eq}
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(\phi_{ki}\)</span> es el coeficiente de la variable dada con el rezago <span class="math inline">\(i\)</span> si el proceso tiene un órden <span class="math inline">\(k\)</span>. Así, los coeficientes <span class="math inline">\(\phi_{kk}\)</span> son los coeficientes de la autocorrelación parcial (considerando un proceso AR(k)). Observemos que la autocorrelaicón parcial mide la correlación entre <span class="math inline">\(X_t\)</span> y <span class="math inline">\(X_{t-k}\)</span> que se mantiene cuando el efecto de las variables <span class="math inline">\(X_{t-1}\)</span>, <span class="math inline">\(X_{t-2}\)</span>, <span class="math inline">\(\ldots\)</span> y <span class="math inline">\(X_{t-k-1}\)</span> en <span class="math inline">\(X_{t}\)</span> y <span class="math inline">\(X_{t-k}\)</span> ha sido eliminado.</p>
<p>Dada la expresión considerada en la ecuación (), podemos resolver el problema de establecer el valor de cada <span class="math inline">\(\phi_{ki}\)</span> mediante la solución del sistema que se representa en lo siguiente:
<span class="math display">\[\begin{equation}
    \left[ 
    \begin{array}{c}
        \rho(1) \\
        \rho(2) \\
        \vdots \\
        \rho(k)
    \end{array} 
    \right]
    = 
    \left[ 
    \begin{array}{c c c c}
        1 &amp; \rho(1) &amp; \ldots &amp; \rho(k - 1)\\
        \rho(1) &amp; 1 &amp; \ldots &amp; \rho(k - 2)\\
        \rho(2) &amp; \rho(1) &amp; \ldots &amp; \rho(k - 3)\\
        \vdots &amp; \vdots &amp; \ldots &amp; \vdots\\
        \rho(k - 1) &amp; \rho(k - 2) &amp; \ldots &amp; 1\\
    \end{array} 
    \right]
    \left[ 
    \begin{array}{c}
        \phi_{k1} \\
        \phi_{k2} \\
        \phi_{k3} \\
        \vdots \\
        \phi_{kk} \\
    \end{array} 
    \right]
\end{equation}\]</span></p>
<p>Del cual se puede derivar una solución, resoviendo por el método de cramer, o cualquier otro método que consideremos y que permita calcular la solución de sistemas de ecuaciones.</p>
<p>Posterior al análisis analítico platearemos un enfoque para interpretar las funciones de autocorrelación y autocorrelación parcial. Este enfoque pretende aportar al principio de parcimonia, en el cual podemos identificar el número de parámetros que posiblemente puede describir mejor a la serie en un modelo ARMA(p, q).</p>
En el Cuadro  se muestra un resumen de las caranterísticas que debemos observar para determinar el número de parámetros de cada uno de los componentes AR y MA. Lo anterior por observación de las funciones de autocorrelación y autocorrelación parcial. Este enfoque no es el más formal, más adelante implemtaremos uno más formal y que puede ser más claro de cómo determinar el númeto de parámetros.

Continuando con el ejemplo en la Figura () mostramos tanto la Función de Autocorrelación como la Función de Autocorrelación Parcial. En esta identificamos que ambas gráficas muestran que el modelo que explica a la variable <span class="math inline">\(DLPaxNal_t\)</span> tiene tanto componentes AR como MA. Sin embargo, dado lo errático del comportamiento de ambas funciones, resulta complicado determinar cuál sería un buen número de parametros <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span> a considerar en el <span class="math inline">\(ARMA(p,q)\)</span>. Por esta razón a continuación platearemos algunas pruebas más formales para determinar dichos parámetros.

</div>
<div id="selección-de-las-constantes-p-q-d-en-un-arp-un-maq-un-armap-q-o-un-arimap-d-q" class="section level2">
<h2><span class="header-section-number">4.5</span> Selección de las constantes p, q, d en un AR(p), un MA(q), un ARMA(p, q) o un ARIMA(p, d, q)</h2>
<p>Respecto de cómo estimar un proceso ARMA(p, q) –en general utilizaremos este modelo para discutir, pero lo planteado en esta sección es igualmente aplicable en cualquier otro caso como aquellos modelos que incluyen variables exogénas– existen diversas formas de estimar los paramétros <span class="math inline">\(a_i\)</span> y <span class="math inline">\(b_i\)</span>: i) por máxima verosimilitd y ii) por mínimos cuadrados órdinarios. El primer caso requiere que conozcamos la distribución del proceso aleatorio <span class="math inline">\(U_t\)</span>. El segundo, por el contrario, no requiere el mismo supuesto. No obstante, para el curso utilizaremos el método de máxima verosimilitud.</p>
<p>Otra duda que debe quedar hasta el momento es ¿cómo determinar el orden <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span> del proceso ARMA(p, q)? La manera más convencional y formal que existe para tal efecto es utilizar los criterios de información. Así, el orden se elije de acuerdo a aquel críterio de información que resulta ser el mínimo. En el caso de <span class="math inline">\(d\)</span> se selecciona revisando la gráfica que parezca más estacionaria–más adelante mostraremos un proceso más formal para su selección.</p>
Los criterios de información más comunes son los siguientes:

<p>Donde <span class="math inline">\(\hat{U}_t^{(p)}\)</span> son los residuales estimados mediante un proceso ARIMA y <span class="math inline">\(m\)</span> es el número de parametros estimados: <span class="math inline">\(m = p + q + 0 + 1\)</span> (ya que asumimos que <span class="math inline">\(d = 0\)</span>). Una propiedad que no se debe perder de vista es que los criterios de información cumplen la siguiente relación:
<span class="math display">\[\begin{equation}
    orden(SC) \leq orden(HQ) \leq orden(AIC)
\end{equation}\]</span></p>
<p>Por esta razón, durante el curso solo utilizaremos el criterio se Akaike para determinar el orden óptimo del proceso ARMA, ya que ello garantiza el orden más grande posible.</p>
<p>Ahora veamos un ejemplo de estimación del número de rezagos optimo de un <span class="math inline">\(ARMA(p, q)\)</span>. Retomemos la serie en diferencias logarítmicas de los pasajeros en vuelos nacionales de salidas, pero ahora incluiremos la variables exógenas de dummies estacionales: enero, febrero, julio y diciembre.</p>
<p>Como mencionamos, las gráficas de las funciones de autocorrelación permiten observar el valor de la correlación existente entre la variable en el momento <span class="math inline">\(t\)</span> con cada uno de los rezagos. Incluso la Función de Autocorrelación Parcial puede ayudar a determinar el número máximo de rezagos que se debe incluir en el proceso <span class="math inline">\(AR(p)\)</span>. No obstante, una métrica más formal es el uso de los criterios de información. En nuestro caso, dado lo discutido, sólo utilizareemos el criterio de Akaike.</p>
Al respecto, en el Cuadro () reportamos el criterio de Akaike que resultan de aplicar dicho criterio a los residuales resultantes de cada combinación de procesos <span class="math inline">\(ARMA(p, q)\)</span>. La forma de escoger será aquel modelo que reporta el criterio de Akaike menor. En la cuarta columna de la tabla se señala el valor del criterio de información que resulta ser el mínimo de todos los posibles.

<p>El Cuadro () reporta los resultados para 36 diferentes modelos, todos incluyen variables exogenas. Como resultado del análisis concluimos que el modelo más adecuado es el 24, el cual considera un <span class="math inline">\(ARMA(4, 6)\)</span>, con variables dummies para controlar la estacionalidad de los meses de enero, febrero, julio y diciembre. En el Cuadro () mostramos los resutados del modelo.</p>
No obstante, una inspección de los residuales del modelo nos permite sospechar que requiere de incluir un par de dummies más. Ambas, asociadas con la caída del transporte aéreo en 2009, principalmente asociado con la crisis mundial de ese año. La Figura () muestra los residuales mencionados.

Una vez incluidas dos dummies más para mayo y junio de 2009, analizamos un total de 36 modelos ARMA y determinamos que el orden que minimizza el criterio de Akaike es un <span class="math inline">\(ARMA(4, 6)\)</span>. El Cuadro () muestra los resultados para este nuevo modelo. No lo motramos en esta sección, pero ambos modelos reportados tienen raices de sus respectivos polinomios característicos menores a 1 en valor absoluto. En la Figura  mostramos los residuales ahora ajustados por las dummies de mayo y junio de 2009.


</div>
<div id="pronósticos" class="section level2">
<h2><span class="header-section-number">4.6</span> Pronósticos</h2>
<p>Para pronósticar el valor de la serie es necesario determinar cuál es el valor esperado de la serie en un momento <span class="math inline">\(t + \tau\)</span> condicional en que ésta se comporta como un <span class="math inline">\(AR(p)\)</span>, un <span class="math inline">\(MA(q)\)</span> o un <span class="math inline">\(ARMA(p, q)\)</span> y a que los valores antes de <span class="math inline">\(t\)</span> están dados. Por lo que el pronóstico de la serie estará dado por una expresión:
<span class="math display">\[\begin{eqnarray}
    \mathbb{E}_t[X_{t+\tau}] = \delta + a_1 \mathbb{E}_t[X_{t+\tau-1}] + a_2 \mathbb{E}_t[X_{t+\tau-2}] + \ldots + + a_p \mathbb{E}_t[X_{t+\tau-p}]
    \label{ARMApq_For}
\end{eqnarray}\]</span></p>
<p>Lo anterior para todo <span class="math inline">\(\tau = 0, 1, 2, \ldots\)</span> y considerando que los componentes MA(q) en la eucación () son cero dado que para todo valor <span class="math inline">\(t + \tau\)</span> es cierto que <span class="math inline">\(\mathbb{E}_t[U_{t+\tau}]\)</span>.</p>
Continuando con el ejemplo, en la Figura () mostramos el resultado del pronóstico de la serie a partir del modelo ARMA(4, 6) que hemos discutido anteriormente.


</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modelos-de-series-de-tiempo-estacionarias.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="desestacionalización-y-filtrado-de-series.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Notas de Clase.pdf", "Notas de Clase.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
